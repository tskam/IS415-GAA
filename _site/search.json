[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this section, I will install and load tidyverse and sf packages.\n\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting the Geospatial Data",
    "text": "Plotting the Geospatial Data\n\nplot(mpsz)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate geospatial data wrangling methods to prepare the data for water point mapping study. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data will be used in this take-home exercise. The data can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate sf method, import the shapefile into R and save it in a simple feature data frame format. Note that there are three Projected Coordinate Systems of Nigeria, they are: EPSG: 26391, 26392, and 26303. You can use any one of them.\nUsing appropriate tidyr and dplyr methods, derive the number of functional and non-functional water points at LGA level.\nCombining the geospatial and aspatial data frame into simple feature data frame.\nVisualising the distribution of water point by using appropriate statistical methods."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#getting-started",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#getting-started",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2 Getting started",
    "text": "2 Getting started\nFor the purpose of this in-class exercise, three R packages will be used. They are: sf, tidyverse and funModeling.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the step you had learned, check if these three R packages have been installed in you laptop, if not install the missing R packages. If Yes, launch the R packages into R environment\n\n\n\n\n\nShow the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-geospatial-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-geospatial-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3 Handling Geospatial Data",
    "text": "3 Handling Geospatial Data\n\n3.1 Importing Geospatial\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the step you had learned, import the LGA boundary GIS data of Nigeria downloaded from both sources recommend above.\n\n\n\n3.1.1 The geoBoundaries data set\n\n\n\nShow the code\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n\n3.1.2 The NGA data set\n\n\n\nShow the code\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"nga_admbnda_adm2\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `nga_admbnda_adm2' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\nBy examining both sf dataframe closely, we notice that NGA provide both LGA and state information. Hence, NGA data.frame will be used for the subsequent processing.\n\n\n\n3.2 Importing Aspatial data\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, import the downloaded water point data set into R, at the same time select only water points within Nigeria.\n\n\n\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\n\n\n3.2.1 Converting water point data into sf point features\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, convert the newly extracted wp_NGA into an sf data.frame\n\n\n\n\n\nShow the code\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga\n\n\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, convert the newly extracted wp_NGA into an sf data.frame\n\n\n\n\n\nShow the code\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n\n3.2.2 Transforming into Nigeria projected coordinate system\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, transform the projection from wgs84 to appropriate projected coordinate system of Nigeria.\n\n\n\n\n\nShow the code\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-cleaning",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-cleaning",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4 Geospatial Data Cleaning",
    "text": "4 Geospatial Data Cleaning\nData cleaning is an important step in any data science task including geospatial data science. It is important for us to do our due deligent to check if any data quality issues occured in the data used.\n\n4.1 Excluding redundent fields\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n\nNGA <- NGA %>%\n  select(c(3:4, 8:9))\n\n\n\n\n4.2 Checking for duplicate name\nIt is always important to check for duplicate name in the data main data fields. Using duplicated() of Base R, we can flag out LGA names that might be duplicated as shown in the code chunk below.\n\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\n\nThe printout above shows that there are 6 LGAs with the same name. A Google search using the coordinates showed that there are LGAs with the same name but are located in different states. For instances, there is a Bassa LGA in Kogi State and a Bassa LGA in Plateau State.\nLet us correct these errors by using the code chunk below.\n\n\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "5 Data Wrangling for Water Point Data",
    "text": "5 Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n\nfreq(data = wp_sf,\n     input = '#status_clean')\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\n\nFigure above shows that there are nine classes in the #status_clean fields.\nNext, code chunk below will be used to perform the following data wrangling tasksP - rename() of dplyr package is used to rename the column from #status_clean to status_clean for easier handling in subsequent steps. - select() of dplyr is used to include status_clean in the output sf data.frame. - mutate() and replace_na() are used to recode all the NA values in status_clean into unknown.\n\n\nwp_sf_nga <- wp_sf %>% \n  rename(status_clean = '#status_clean') %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(\n    status_clean, \"unknown\"))\n\n\n\n5.1 Extracting Water Point Data\nNow we are ready to extract the water point data according to their status.\nThe code chunk below is used to extract functional water point.\n\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\n\nThe code chunk below is used to extract nonfunctional water point.\n\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\n\nThe code chunk below is used to extract water point with unknown status.\n\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\n\n\n\n                 status_clean frequency percentage cumulative_perc\n1                  Functional     45883      87.99           87.99\n2 Functional but needs repair      4579       8.78           96.77\n3   Functional but not in use      1686       3.23          100.00\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\n\n\n\n                      status_clean frequency percentage cumulative_perc\n1                   Non-Functional     29385      91.25           91.25\n2 Non-Functional due to dry season      2403       7.46           98.71\n3         Abandoned/Decommissioned       234       0.73           99.44\n4                        Abandoned       175       0.54           99.98\n5 Non functional due to dry season         7       0.02          100.00\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n\n  status_clean frequency percentage cumulative_perc\n1      unknown     10656        100             100\n\n\n\n\n\n5.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n\nNGA_wp <- NGA %>% \n  mutate(`total_wp` = lengths(\n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown)))\n\n\nNotice that four new derived fields have been added into NGA_wp sf data.frame.\n\n\n5.3 Visualing attributes by using statistical graphs\nIn this code chunk below, appropriate functions of ggplot2 package is used to reveal the distribution of total water points by LGA in histogram.\n\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\n\n\n\n\n5.4 Saving the analytical data in rds format\nIn order to retain the sf object structure for subsequent analysis, it is recommended to save the sf data.frame into rds format.\nIn the code chunk below, write_rds() of readr package is used to export an sf data.frame into rds format.\n\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps. For the purpose of this exercise, Nigeria water point data prepared during In-class Exercise 2 will be used.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#getting-started",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#getting-started",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 Installing and loading packages\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap and tidyverse packages into R environment.\n\n\n\n\n\nShow the code\npacman::p_load(tmap, tidyverse, sf)\n\n\n\n\n\n2.2 Importing data\n\n\n\n\n\n\nNote\n\n\n\nImporting NGA_wp.rds created in the previous in-class into R environment.\n\n\n\n\n\nShow the code\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#basic-choropleth-mapping",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#basic-choropleth-mapping",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "3 Basic Choropleth Mapping",
    "text": "3 Basic Choropleth Mapping\n\n3.1 Visualising distribution of non-functional water point\n\n\n\n\n\n\nYour turn\n\n\n\nPlot a choropleth map showing the distribution of non-function water point by LGA\n\n\n\n\n\nShow the code\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\n\n\n\n\nShow the code\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\n\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#choropleth-map-for-rates",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#choropleth-map-for-rates",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "4 Choropleth Map for Rates",
    "text": "4 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\n4.2 Plotting map of rate\n\n\n\n\n\n\nYour turn\n\n\n\nPlot a choropleth map showing the distribution of percentage functional water point by LGA\n\n\n\n\n\nShow the code\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#extreme-value-maps",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#extreme-value-maps",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "5 Extreme Value Maps",
    "text": "5 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n5.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\n\nStep 2: Creating customised classification and extracting values\n\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n5.1.2 Why writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n5.1.3 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n5.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n5.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n5.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n5.2.3 Test drive the newly created function\nLet’s test the newly created function\n\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n\n\n5.2.5 Recode zero\nThe code chunk below is used to recode LGAs with zero total water point into NA.\n\nNGA_wp <- NGA_wp %>%\n  mutate(wp_functional = na_if(\n    total_wp, total_wp < 0))\n\n\nNGA_state <- NGA_wp %>%\n  group_by(ADM1_EN) %>%\n  summarise(total_wp = sum(total_wp),\n            wp_functional = sum(wp_functional),\n            wp_nonfunctional = sum(wp_nonfunctional)) %>%\n  ungroup() %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\nggplot(data = NGA_state,\n       aes(x = pct_nonfunctional,\n           y = reorder(ADM1_EN, pct_nonfunctional))) +\n  geom_col()\n\n\n\n\nreorder(miRNA, -value)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(maptools, sf, raster, spatstat, tmap)\n\nThings to learn from this code chunk."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-data",
    "title": "In-class Exercise 4",
    "section": "Importing Data",
    "text": "Importing Data\n\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex04\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf <- st_read(dsn = \"data\", \n                 layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex04\\data' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex04\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nVisualising the sf layers\nIt is always a good practice to plot the output sf layers on OSM layer to ensure that they have been imported properly and been projected on an appropriate projection system.\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(childcare_sf) +\n  tm_dots(alph = 0.5, \n          size=0.01) +\n  tm_view(set.zoom.limits = c(11,14))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geospatial-data-wrangling",
    "title": "In-class Exercise 4",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nHandling duplicated point events\n\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\nCreating owin object\n\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\n\n\nCombining point events object and owin object\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\nsummary(childcareSG_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 2.063463e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414\n\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/data/stores.html",
    "href": "In-class_Ex/In-class_Ex05/data/stores.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     \n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/data/study_area.html",
    "href": "In-class_Ex/In-class_Ex05/data/study_area.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "In this in-class exercise, you will learn how to perform Local Colocation Quotient Analysis by using convenience store data of Taiwan as a use case."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "title": "In-class Exercise 5",
    "section": "Getting Started",
    "text": "Getting Started\nFor the purpose of this in-class exercise, four R packages will be used. They are:\n\ntidyverse for performing data science tasks,\nsf for importing, managing and processing geospatial data in simple feature data.frame,\ntmap for plotting cartographic quality maps, and\nsfdep for performing geospatia data wrangling and local colocation quotient analysis.\n\n\npacman::p_load(tidyverse, tmap, sf, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-data",
    "title": "In-class Exercise 5",
    "section": "Importing Data",
    "text": "Importing Data\nTwo geospatial data will be used in this hands-on exercise. Both of them are in ESRI shapefile format.\n\nStudy area data\nThis is a polygon features data showing selected towns of Taipei city. The original data set is in geographic coordinate system and st_transform is used to the data set into projected coordinates system\n\nstudyArea <- st_read(dsn = \"data\", \n                 layer=\"study_area\") %>%\n  st_transform(crs = 3829)\n\nReading layer `study_area' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex05\\data' using driver `ESRI Shapefile'\nSimple feature collection with 7 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 121.4836 ymin: 25.00776 xmax: 121.592 ymax: 25.09288\nGeodetic CRS:  TWD97\n\n\n\n\nStores data\nThis is a point features data showing selected towns of Taipei city. The original data set is in geographic coordinate system and st_transform is used to the data set into projected coordinates system\n\nstores <- st_read(dsn = \"data\",\n                  layer = \"stores\") %>%\n  st_transform(crs = 3829)\n\nReading layer `stores' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex05\\data' using driver `ESRI Shapefile'\nSimple feature collection with 1409 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 121.4902 ymin: 25.01257 xmax: 121.5874 ymax: 25.08557\nGeodetic CRS:  TWD97"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualising-the-sf-layers",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualising-the-sf-layers",
    "title": "In-class Exercise 5",
    "section": "Visualising the sf layers",
    "text": "Visualising the sf layers\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\ntm_shape(stores)+ \n  tm_dots(col = \"Name\",\n             size = 0.01,\n             border.col = \"black\",\n             border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12, 16))\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-colocation-quotients-lclq",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-colocation-quotients-lclq",
    "title": "In-class Exercise 5",
    "section": "Local Colocation Quotients (LCLQ)",
    "text": "Local Colocation Quotients (LCLQ)\n\nPreparing nearest neighbours list\nIn the code chunk below, st_knn() of sfdep package is used to determine the k (i.e. 6) nearest neighbours for given point geometry.\n\nnb <- include_self(\n  st_knn(st_geometry(stores), 6))\n\n\n\nComputing kernel weights\nIn the code chunk below, st_kernel_weights() of sfdep package is used to derive a weights list by using a kernel function.\n\nwt <- st_kernel_weights(nb, \n                        stores, \n                        \"gaussian\", \n                        adaptive = TRUE)\n\n\n\n\n\n\n\nNote\n\n\n\n\nan object of class nb e.g. created by using either st_contiguity() or st_knn() is required.\nThe supported kernel methods are: “uniform”, “gaussian”, “triangular”, “epanechnikov”, or “quartic”.\n\n\n\n\n\nPreparing the vector list\nTo compute LCLQ by using sfdep package, the reference point data must be in either character or vector list. The code chunks below are used to prepare two vector lists. One of Family Mart and for 7-11 and are called A and B respectively.\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\nA <- FamilyMart$Name\n\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\n\n\nComputing LCLQ\nIn the code chunk below local_colocation() us used to compute the LCLQ values for each Family Mart point event.\n\nLCLQ <- local_colocation(A, B, nb, wt, 49)\n\n\n\nJoining output table\nBefore we can plot the LCLQ values their p-values, we need to join the output of local_colocation() to the stores sf data.frame. However, a quick check of LCLQ data-frame, we can’t find any field can be used as the join field. As a result, cbind() of Base R is useed.\n\nLCLQ_stores <- cbind(stores, LCLQ)\n\n\n\nPlotting LCLQ values\nIn the code chunk below, tmap functions are used to plot the LCLQ analysis.\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\ntm_shape(LCLQ_stores)+ \n  tm_dots(col = \"X7.Eleven\",\n             size = 0.01,\n             border.col = \"black\",\n             border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12, 16))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#getting-started",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#getting-started",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\nShow the code\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#the-data",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#the-data",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nImporting geospatial data\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\nShow the code\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting attribute table\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\nShow the code\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\nCombining both data frame by using left join\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)\n\n\n\n\nShow the code\nhunan_GDPPC <- left_join(hunan, hunan2012) %>%\n  select(1:4, 7, 15)\n\n\n\n\nPlotting a choropleth map\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\nThe choropleth should look similar to ther figure below.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#deriving-contiguity-spatial-weights",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#deriving-contiguity-spatial-weights",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "Deriving Contiguity Spatial Weights",
    "text": "Deriving Contiguity Spatial Weights\nBy and large, there are two types of spatial weights, they are contiguity wights and distance-based weights. In this section, you will learn how to derive contiguity spatial weights by using sfdep.\nTwo steps are required to derive a contiguity spatial weights, they are:\n\nidentifying contiguity neighbour list by st_contiguity() of sfdep package, and\nderiving the contiguity spatial weights by using st_weights() of sfdep package\n\nIn this section, we will learn how to derive the contiguity neighbour list and contiguity spatial weights separately. Then, we will learn how to combine both steps into a single process.\n\nIdentifying contiguity neighbours: Queen’s method\nIn the code chunk below st_contiguity() is used to derive a contiguity neighbour list by using Queen’s method.\n\nnb_queen <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, queen argument is TRUE. If you do not specify queen = FALSE, this function will return a list of first order neighbours by using the Queen criteria. Rooks method will be used to identify the first order neighbour if queen = FALSE is used.\n\n\nThe code chunk below is used to print the summary of the first lag neighbour list (i.e. nb) .\n\nsummary(nb_queen$nb)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan province. The most connected area unit has 11 neighbours. There are two are units with only one neighbour.\nTo view the content of the data table, you can either display the output data frame on RStudio data viewer or by printing out the first ten records by using the code chunk below.\n\nnb_queen\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb   NAME_2  ID_3    NAME_3   ENGTYPE_3\n1                 2, 3, 4, 57, 85  Changde 21098   Anxiang      County\n2               1, 57, 58, 78, 85  Changde 21100   Hanshou      County\n3                     1, 4, 5, 85  Changde 21101    Jinshi County City\n4                      1, 3, 5, 6  Changde 21102        Li      County\n5                     3, 4, 6, 85  Changde 21103     Linli      County\n6                4, 5, 69, 75, 85  Changde 21104    Shimen      County\n7                  67, 71, 74, 84 Changsha 21109   Liuyang County City\n8       9, 46, 47, 56, 78, 80, 86 Changsha 21110 Ningxiang      County\n9           8, 66, 68, 78, 84, 86 Changsha 21111 Wangcheng      County\n10 16, 17, 19, 20, 22, 70, 72, 73 Chenzhou 21112     Anren      County\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\nThe print shows that polygon 1 has five neighbours. They are polygons number 2, 3, 4, 57,and 85.\nYou can reveal the county name of the five neighbouring polygons of popygon No. 1 (i.e. Anxiang) by using the code chunk below.\n\nnb_queen$County[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\n\n\nIdentify contiguity neighbours: Rooks’ method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity neighbour list using Rooks’ method.\n\n\n\n\nShow the code\nnb_rook <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         .before = 1)\n\n\n\n\nIdentifying higher order neighbors\nThere are times that we need to identify high order contiguity neighbours. To accomplish the task, st_nb_lag_cumul() should be used as shown in the code chunk below.\n\nnb2_queen <-  hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\nNote that if the order is 2, the result contains both 1st and 2nd order neighbors as shown on the print below.\n\nnb2_queen\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#deriving-contiguity-weights-queens-method",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#deriving-contiguity-weights-queens-method",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "Deriving contiguity weights: Queen’s method",
    "text": "Deriving contiguity weights: Queen’s method\nNow, you are ready to compute the contiguity weights by using st_weights() of sfdep package.\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\nDeriving contiguity weights: Rooks method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity weights using Rooks method.\n\n\n\n\nShow the code\nwm_r <- hunan %>%\n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         wt = st_weights(nb),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#distance-based-weights",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#distance-based-weights",
    "title": "In-class Exercise 6: Spatial Weights - sfdep methods",
    "section": "Distance-based Weights",
    "text": "Distance-based Weights\nThere are three popularly used distance-based spatial weights, they are:\n\nfixed distance weights,\nadaptive distance weights, and\ninverse distance weights (IDW).\n\n\nDeriving fixed distance weights\nBefore we can derive the fixed distance weights, we need to determine the upper limit for distance band by using the steps below:\n\ngeo <- sf::st_geometry(hunan_GDPPC)\nnb <- st_knn(geo, longlat = TRUE)\ndists <- unlist(st_nb_dists(geo, nb))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_nb_dists() of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation’s neighbors list.\nunlist() of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.\n\n\n\nNow, we will go ahead to derive summary statistics of the nearest neighbour distances vector (i.e. dists) by usign the coced chunk below.\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nThe summary statistics report above shows that the maximum nearest neighbour distance is 65.80km. By using a threshold value of 66km will ensure that each area will have at least one neighbour.\nNow we will go ahead to compute the fixed distance weights by using the code chunk below.\n\nwm_fd <- hunan_GDPPC %>%\n  mutate(nb = st_dist_band(geometry,\n                           upper = 66),\n               wt = st_weights(nb),\n               .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_dists_band() of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\n\n\n\n\nDo It Yourself\n\n\n\nUsing the steps you learned in previous section, examine the data frame of the fixed distance weights.\n\n\n\n\nDeriving adaptive distance weights\nIn this section, you will derive an adaptive spatial weights by using the code chunk below.\n\nwm_ad <- hunan_GDPPC %>% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_knn() of sfdep is used to identify neighbors based on k (i.e. k = 8 indicates the nearest eight neighbours). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\nCalculate inverse distance weights\nIn this section, you will derive an inverse distance weights by using the code chunk below.\n\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_contiguity() of sfdep is used to identify the neighbours by using contiguity criteria. The output is a list of neighbours (i.e. nb).\nst_inverse_distance() is then used to calculate inverse distance weights of neighbours on the nb list."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Chapter 9: Global Measures of Spatial Autocorrelation and Chapter 10: Local Measures of Spatial Autocorrelation. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#getting-started",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\n\nShow the code\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#the-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#the-data",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nImporting geospatial data\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\n\nShow the code\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nImporting attribute table\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\n\nShow the code\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nCombining both data frame by using left join\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)\n\n\n\n\n\nShow the code\nhunan_GDPPC <- left_join(hunan, hunan2012) %>%\n  select(1:4, 7, 15)\n\n\n\n\n\nPlotting a choropleth map\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\nThe choropleth should look similar to the figure below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#deriving-contiguity-spatial-weights",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#deriving-contiguity-spatial-weights",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Deriving Contiguity Spatial Weights",
    "text": "Deriving Contiguity Spatial Weights\nBy and large, there are two types of spatial weights, they are contiguity wights and distance-based weights. In this section, you will learn how to derive contiguity spatial weights by using sfdep.\nTwo steps are required to derive a contiguity spatial weights, they are:\n\nidentifying contiguity neighbour list by st_contiguity() of sfdep package, and\nderiving the contiguity spatial weights by using st_weights() of sfdep package\n\nIn this section, we will learn how to derive the contiguity neighbour list and contiguity spatial weights separately. Then, we will learn how to combine both steps into a single process.\n\nIdentifying contiguity neighbours: Queen’s method\nIn the code chunk below st_contiguity() is used to derive a contiguity neighbour list by using Queen’s method.\n\nnb_queen <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, queen argument is TRUE. If you do not specify queen = FALSE, this function will return a list of first order neighbours by using the Queen criteria. Rooks method will be used to identify the first order neighbour if queen = FALSE is used.\n\n\nThe code chunk below is used to print the summary of the first lag neighbour list (i.e. nb) .\n\nsummary(nb_queen$nb)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan province. The most connected area unit has 11 neighbours. There are two are units with only one neighbour.\nTo view the content of the data table, you can either display the output data frame on RStudio data viewer or by printing out the first ten records by using the code chunk below.\n\nnb_queen\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb   NAME_2  ID_3    NAME_3   ENGTYPE_3\n1                 2, 3, 4, 57, 85  Changde 21098   Anxiang      County\n2               1, 57, 58, 78, 85  Changde 21100   Hanshou      County\n3                     1, 4, 5, 85  Changde 21101    Jinshi County City\n4                      1, 3, 5, 6  Changde 21102        Li      County\n5                     3, 4, 6, 85  Changde 21103     Linli      County\n6                4, 5, 69, 75, 85  Changde 21104    Shimen      County\n7                  67, 71, 74, 84 Changsha 21109   Liuyang County City\n8       9, 46, 47, 56, 78, 80, 86 Changsha 21110 Ningxiang      County\n9           8, 66, 68, 78, 84, 86 Changsha 21111 Wangcheng      County\n10 16, 17, 19, 20, 22, 70, 72, 73 Chenzhou 21112     Anren      County\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\nThe print shows that polygon 1 has five neighbours. They are polygons number 2, 3, 4, 57,and 85.\nYou can reveal the county name of the five neighbouring polygons of popygon No. 1 (i.e. Anxiang) by using the code chunk below.\n\nnb_queen$County[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\n\n\nIdentify contiguity neighbours: Rooks’ method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity neighbour list using Rooks’ method.\n\n\n\n\nShow the code\nnb_rook <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         .before = 1)\n\n\n\n\nIdentifying higher order neighbors\nThere are times that we need to identify high order contiguity neighbours. To accomplish the task, st_nb_lag_cumul() should be used as shown in the code chunk below.\n\nnb2_queen <-  hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\nNote that if the order is 2, the result contains both 1st and 2nd order neighbors as shown on the print below.\n\nnb2_queen\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#deriving-contiguity-weights-queens-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#deriving-contiguity-weights-queens-method",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Deriving contiguity weights: Queen’s method",
    "text": "Deriving contiguity weights: Queen’s method\nNow, you are ready to compute the contiguity weights by using st_weights() of sfdep package.\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\nDeriving contiguity weights: Rooks method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity weights using Rooks method.\n\n\n\n\nShow the code\nwm_r <- hunan %>%\n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         wt = st_weights(nb),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#distance-based-weights",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#distance-based-weights",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Distance-based Weights",
    "text": "Distance-based Weights\nThere are three popularly used distance-based spatial weights, they are:\n\nfixed distance weights,\nadaptive distance weights, and\ninverse distance weights (IDW).\n\n\nDeriving fixed distance weights\nBefore we can derive the fixed distance weights, we need to determine the upper limit for distance band by using the steps below:\n\ngeo <- sf::st_geometry(hunan_GDPPC)\nnb <- st_knn(geo, longlat = TRUE)\ndists <- unlist(st_nb_dists(geo, nb))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_nb_dists() of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation’s neighbors list.\nunlist() of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.\n\n\n\nNow, we will go ahead to derive summary statistics of the nearest neighbour distances vector (i.e. dists) by usign the coced chunk below.\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nThe summary statistics report above shows that the maximum nearest neighbour distance is 65.80km. By using a threshold value of 66km will ensure that each area will have at least one neighbour.\nNow we will go ahead to compute the fixed distance weights by using the code chunk below.\n\nwm_fd <- hunan_GDPPC %>%\n  dplyr::mutate(nb = st_dist_band(geometry,\n                                  upper = 66),\n               wt = st_weights(nb),\n               .before = 1)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nst_dists_band() of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\n\n\n\n\nDo It Yourself\n\n\n\nUsing the steps you learned in previous section, examine the data frame of the fixed distance weights."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#deriving-adaptive-distance-weights",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#deriving-adaptive-distance-weights",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Deriving adaptive distance weights",
    "text": "Deriving adaptive distance weights\nIn this section you will learn how to derive an adaptive distance weights.\n\nwm_ad <- hunan_GDPPC %>% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n\nCalculate inverse distance weights\n\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#computing-local-morans-i",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\n\n\nlisa <- wm_q %>% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_moran)\nlisa\n\nSimple feature collection with 88 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 21\n         ii        eii   var_ii    z_ii    p_ii p_ii_…¹ p_fol…² skewn…³ kurtosis\n      <dbl>      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 -0.00147  0.00177    4.18e-4 -0.158  0.874      0.82    0.41  -0.812  0.652  \n 2  0.0259   0.00641    1.05e-2  0.190  0.849      0.96    0.48  -1.09   1.89   \n 3 -0.0120  -0.0374     1.02e-1  0.0796 0.937      0.76    0.38   0.824  0.0461 \n 4  0.00102 -0.0000349  4.37e-6  0.506  0.613      0.64    0.32   1.04   1.61   \n 5  0.0148  -0.00340    1.65e-3  0.449  0.654      0.5     0.25   1.64   3.96   \n 6 -0.0388  -0.00339    5.45e-3 -0.480  0.631      0.82    0.41   0.614 -0.264  \n 7  3.37    -0.198      1.41e+0  3.00   0.00266    0.08    0.04   1.46   2.74   \n 8  1.56    -0.265      8.04e-1  2.04   0.0417     0.08    0.04   0.459 -0.519  \n 9  4.42     0.0450     1.79e+0  3.27   0.00108    0.02    0.01   0.746 -0.00582\n10 -0.399   -0.0505     8.59e-2 -1.19   0.234      0.28    0.14  -0.685  0.134  \n# … with 78 more rows, 12 more variables: mean <fct>, median <fct>,\n#   pysal <fct>, nb <nb>, wt <list>, NAME_2 <chr>, ID_3 <int>, NAME_3 <chr>,\n#   ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>, geometry <POLYGON [°]>, and\n#   abbreviated variable names ¹​p_ii_sim, ²​p_folded_sim, ³​skewness\n\n\n\n\nVisualising local Moran’s I\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\nVisualising p-value of local Moran’s I\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nVisuaising local Moran’s I and p-value\nFor effective comparison, you can plot both maps next to each other as shown below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 <- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 <- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#visualising-local-morans-i-1",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#visualising-local-morans-i-1",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Visualising local Moran’s I",
    "text": "Visualising local Moran’s I\n\n\nlisa_sig <- lisa  %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, “sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#installing-and-loading-the-r-packages",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Installing and Loading the R Packages",
    "text": "Installing and Loading the R Packages\nAs usual, p_load() of pacman package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\nThree R packages are need for this in-class exercise, they are: sf, sfdep and tidyverse.\n\n\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#importing-geospatial-data",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nIn the code chunk below, st_read() of sf package is used to import Hunan shapefile into R.\n\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\IS415-GAA\\In-class_Ex\\In-class_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\nNow, we plot the study are by using tmap function as shown in the code chunk below.\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(hunan) +\n  tm_polygons(alpha = 0.05)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#importing-attribute-table",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#importing-attribute-table",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Importing attribute table",
    "text": "Importing attribute table\nIn the code chunk below, read_csv() of readr is used to import Hunan_2012.csv into R.\n\n\nGDPPC <- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\nRows: 1496 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): County\ndbl (2): Year, GDPPC\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analyticvs and Applications.\nThis is the course website of IS415 I study this term. You will find my course work on this website."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Global Measures of Spatial Association",
    "text": "Global Measures of Spatial Association\n\nStep 1: Deriving contiguity weights: Queen’s method\n\n\n\n\n\n\nDo it Yourself!\n\n\n\nUsing the steps you learned in previous lesson, derive a Queen’s contiguity weights by using appropriate spdep and tidyverse functions.\n\n\n\n\nDeriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\nComputing Global Moran’ I\n\n\nmoranI <- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n\n\nPerforming Global Moran’I test\n\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\nPerforming Global Moran’I permutation test\n\nset.seed(1234)\n\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#visualising-local-morans-i",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#visualising-local-morans-i",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Visualising local Moran’s I",
    "text": "Visualising local Moran’s I\n\nlisa_sig <- lisa  %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#computing-gi",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#computing-gi",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Computing Gi*",
    "text": "Computing Gi*\n\n\ngi_stars <- GDPPC_nb %>% \n  group_by(Year) %>% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %>% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#arrange-to-show-significant-emerging-hotcold-spots",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#arrange-to-show-significant-emerging-hotcold-spots",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Arrange to show significant emerging hot/cold spots",
    "text": "Arrange to show significant emerging hot/cold spots\n\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#performing-emerging-hotspot-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#performing-emerging-hotspot-analysis",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Performing Emerging Hotspot Analysis",
    "text": "Performing Emerging Hotspot Analysis\n\n\nehsa <- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)\n\n\n\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#visualising-ehsa",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07_EHSA.html#visualising-ehsa",
    "title": "In-class Exercise 7: Emerging Hot Spot Analysis: sfdep methods",
    "section": "Visualising EHSA",
    "text": "Visualising EHSA\n\nlisa_sig <- lisa  %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#hot-spot-and-cold-spot-area-analysis",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#hot-spot-and-cold-spot-area-analysis",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#computing-local-morans-i-1",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#computing-local-morans-i-1",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\n\n\nHCSA <- wm_q %>% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 16 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 17\n    gi_star   e_gi     var_gi  p_value p_sim p_fol…¹ skewn…² kurto…³ nb    wt   \n      <dbl>  <dbl>      <dbl>    <dbl> <dbl>   <dbl>   <dbl>   <dbl> <nb>  <lis>\n 1 -0.00567 0.0115 0.00000812  9.95e-1  0.82    0.41   1.03    1.23  <int> <dbl>\n 2 -0.235   0.0110 0.00000581  8.14e-1  1       0.5    0.912   1.05  <int> <dbl>\n 3  0.298   0.0114 0.00000776  7.65e-1  0.7     0.35   0.455  -0.732 <int> <dbl>\n 4  0.145   0.0121 0.0000111   8.84e-1  0.64    0.32   0.900   0.726 <int> <dbl>\n 5  0.356   0.0113 0.0000119   7.21e-1  0.64    0.32   1.08    1.31  <int> <dbl>\n 6 -0.480   0.0116 0.00000706  6.31e-1  0.82    0.41   0.364  -0.676 <int> <dbl>\n 7  3.66    0.0116 0.00000825  2.47e-4  0.02    0.01   0.909   0.664 <int> <dbl>\n 8  2.14    0.0116 0.00000714  3.26e-2  0.16    0.08   1.13    1.48  <int> <dbl>\n 9  4.55    0.0113 0.00000656  5.28e-6  0.02    0.01   1.36    4.14  <int> <dbl>\n10  1.61    0.0109 0.00000341  1.08e-1  0.18    0.09   0.269  -0.396 <int> <dbl>\n# … with 78 more rows, 7 more variables: NAME_2 <chr>, ID_3 <int>,\n#   NAME_3 <chr>, ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>,\n#   geometry <POLYGON [°]>, and abbreviated variable names ¹​p_folded_sim,\n#   ²​skewness, ³​kurtosis\n\n\n\n\nVisualising Gi*\n\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\nVisualising p-value of HCSA\n\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nVisuaising local HCSA\nFor effective comparison, you can plot both maps next to each other as shown below.\n\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 <- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 <- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#visualising-local-morans-i-2",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-GLSA.html#visualising-local-morans-i-2",
    "title": "In-class Exercise 7: Global and Local Measures of Spatial Association - sfdep methods",
    "section": "Visualising local Moran’s I",
    "text": "Visualising local Moran’s I\n\n\nlisa_sig <- lisa  %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n\nDeriving contiguity weights: Rooks method\n\n\n\n\n\n\nDo It Yourself!\n\n\n\nUsing the steps you just learned, derive a contiguity weights using Rooks method.\n\n\n\n\nShow the code\nwm_r <- hunan %>%\n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         wt = st_weights(nb),\n         .before = 1)"
  }
]